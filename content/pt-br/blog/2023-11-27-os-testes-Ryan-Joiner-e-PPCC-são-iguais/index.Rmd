---
title: "Os testes Ryan-Joiner e PPCC são iguais?"
authors: [Anderson Canteli]
banner: img/banners/banner-1.jfif
date: 2023-11-27T22:28:00-03:00
categories: ["data analisys"]
tags: ["normality", "normtest", "Python", "Ryan-Joiner", "Looney-Gulledge", "PPCC"]
mathjax: true
bibliography: [RyanJoiner1976.bib, LooneyGulledge1985.bib, Filliben1975.bib, ShapiroWilk1965.bib, Blom1958.bib]

---

<!-- Ajustando configurações iniciais -->


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(knitr)
library(kableExtra)
library(reticulate)
Sys.setenv("RETICULATE_PYTHON" = "C:/Users/ander/Repositorios/puzzle-in-a-mug/blog/venv/Scripts/python.exe")
reticulate::repl_python()

```



<!-- Setando valores iniciais nlolo r -->


```{r, echo=FALSE}
n_min <- 3
n_max <- 10
```



<!-- Importações no Pyton -->

```{python, echo=FALSE}
from normtest import ryan_joiner, filliben
from normtest.utils.critical_values import LOONEY_GULLEDGE_CRITICAL
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
```


<!-- Setando valores iniciais em Python -->


```{python, echo=FALSE}
n_min = int(r.n_min)
n_max = int(r.n_max)
alpha = 0.05
n_sample_name = "Tamanho amostral"
lg_name = "Looney-Gulledge"
rj_name = "Ryan-Joiner"
diff_name = "Diferença"
alpha_name = "Alfa"
critical_name = "Valores críticos"
percentage_name = "Porcentagem (%)"
desv_pad_name = "Desvio padrão*"
diff_quadract = "Diferença quadrática média"
positive_name, negative_name, zero_name = "Positivo", "Negativo", "Nulo"
alphas = [0.01, 0.05, 0.10]
```


<!-- Calculos em Python -->

```{python, echo=FALSE}
all_values = []
rj_criticals = []
lg_criticals = []
diffs = []

for alpha_value in alphas:
    rj_critico = []
    for i in LOONEY_GULLEDGE_CRITICAL["n"][3:]:
        rj_critico.append(round(ryan_joiner._critical_value(i, alpha=alpha_value), 3))
    rj_critico = np.array(rj_critico)
    lg_critico = np.array(LOONEY_GULLEDGE_CRITICAL[alpha_value][3:])

    dff = rj_critico - lg_critico
    all_values.append(
        pd.DataFrame(
            {
                n_sample_name: LOONEY_GULLEDGE_CRITICAL["n"][3:],
                rj_name: lg_critico,
                lg_name: rj_critico,
                diff_name: dff,
                alpha_name: [alpha_value] * dff.size,
            }
        )
    )
    diffs.append(
        pd.DataFrame(
            {
                alpha_name: [alpha_value],
                diff_name: [np.square(rj_critico - lg_critico).sum() / rj_critico.size],
            }
        )
    )


### Tabela com valores críticos entre n_min e n__max
df = pd.concat(all_values)
df_critical_table = df[df[alpha_name] == alpha].copy()
df_critical_table.drop(alpha_name, inplace=True, axis=1)
df_critical_table = df_critical_table.loc[
    df_critical_table[n_sample_name].isin(np.arange(n_min, n_max + 1))
]


# Tabela de diferenças quadraticas diferenças quadraticas

df_quadract = []
for alpha_value in alphas:
    df_aux = df[[diff_name, alpha_name]].copy()
    df_aux = df_aux[df_aux[alpha_name] == alpha_value]

    df_quadract.append(
        pd.DataFrame(
            {
                alpha_name: [alpha_value],
                diff_quadract: [np.square(df_aux[diff_name]).sum() / df_aux.shape[0]],
            }
        )
    )

df_quadract = pd.concat(df_quadract, ignore_index=True)

```


<details>
<summary class="pointer"><h3>Spoiler</h3></summary>
<hr>
<blockquote>

<h4> Os testes de Ryan-Joiner e PPCC sáo iguais?</h4>

Sim e não. Eles são uns 99,98% iguais.

</blockquote >

</details>



## Motivação

Este texto foi escrito durante estudo e implementação do teste de PPCC no pacote [`normtest`](https://normtest.readthedocs.io/en/latest/), onde percebi a semelhança entre os testes, especialmente em relação aos valores críticos. Para verificar o quão parecido são os valores, resolvi encarar o problema tomando um cafezinho.


## Introdução

Quando você começa a estudar testes de Normalidade, invariavelmente irá se deparar com um tipo de teste baseado no *coeficiente de correlação* (vulgo $r_{pearson}$). Estes testes prometem ser simples e ao mesmo tempo poderosos, entregando resultados similares ao teste de @ShapiroWilk1965. Este tipo de teste (provavelmente) foi introduzido por @Filliben1975,  com variações propostas por @RyanJoiner1976 e mais tarde por @LooneyGulledge1985 (o teste PPCC).


A ideia deste tipo de teste é realmente simples: comparar os dados experimentais com o que seria esperado para a distribuição Normal. Geralmente, plotamos o gráfico dos dados ordenados *versus* os valores Normais. Se o gráfico apresentar comportamento linear, quer dizer que existe correlação entre os dados experimentais e a distribuição Normal, e, portanto, os dados podem ser considerados com distribuição Normal. Como o comportamento esperado é linear, o coeficiente de correlação surge naturalmente como uma medida de Normalidade (e.g, a estatística do teste).

A diferença entre estes testes está na forma que se obtém os valores da distribuição Normal, o que não é uma tarefa simples. Porém, existem algumas equações que fornecem resultados adequados (@Blom1958). @Filliben1975 propôs o uso das medianas uniformes, que são estimadas através da seguinte relação:

<blockquote>

$$\begin{split}m_{i} = \begin{cases}1-0.5^{1/n} & i = 1\\ \frac{i-0.3175}{n+0.365} & i = 2, 3,  \ldots , n-1 \\ 0.5^{1/n}& i=n \end{cases}\end{split}$$

onde $n$ é o tamanho da amostra e $i$ é a i-ésima observação.

</blockquote>

As ordens uniformes devem ser transformadas para a escala Normal $z_i$:

<blockquote>
$$z_{i} = \phi^{-1} \left(m_{i} \right)$$

onde $\phi^{-1}$ é a inversa da distribuição Normal padrão.

</blockquote>


Os valores de $z_i$ são plotados com os dados experimentais ordenados, obtendo um gráfico com o perfil linear, como o apresentado na Figura \@ref(fig:fillibenexample).




```{python fillibenexample, fig.cap='Exemplo do gráfico de correlação esperado para uma distribuição Normal.', echo=FALSE}
data = stats.norm.rvs(loc=0, scale=1, size=30, random_state=42)
fig, ax = plt.subplots(figsize=(5, 4))
ax = filliben.correlation_plot(ax, data)
# plt.savefig("correlation_plot.png")
fig.tight_layout()
plt.show()
```

<br>

Se a correlação for forte o suficiente (e.g., se o comportamento dos dados for parecido com uma reta), os dados podem ser considerados com distribuição Normal. A estatística do teste é o coeficiente de correlação, que deve ser comparado com críticos para a conclusão do teste. O artigo original trás os valores críticos para diversos níveis de significância em função do tamanho amostral (@Filliben1975).

A hipótese nula do teste ($H_0$) é de que os dados seguem a distribuição Normal, e vamos procurar por evidências do contrário ($H_1$). Ou seja:

<blockquote>
* $H_0$: Os dados foram amostrado da distribuição Normal;
* $H_1$: Os dados foram amostrados de uma distribuição diferente da Normal;
</blockquote>

A conclusão é feita comparando o valor crítico ($R_{critico}$) com a estatística do teste ($R_p$). Caso a estatística seja maior ou igual ao valor crítico, nós falhamos em rejeitar a hipótese de normalidade dos dados. Caso contrário, nós rejeitamos a hipótes nula. Assim:

<blockquote>
* Se $R_p \geq R_{critico}$ dados são, pelo menos aproximadamente, Normais;
* Se $R_p < R_{critico}$ dados não são Normais;
</blockquote>

<br>

## Relação entre os testes de Ryan-Joiner e PPCC


Os testes de Ryan-Joiner e PPCC (*Probability Plot Correlation Coefficient*, que será chamado como teste de Looney-Gulledge a partir de agora neste texto) utilizam a mesma lógica vista acima, comparando o coeficiente de correlação com valores críticos. Porém, utilizam a estatística de ordem Normal ao invés da medianas uniformes, que são estimadas de forma um pouco diferente. As equações utilizadas apresentam o perfil:

<blockquote>

$$p_{i} = \frac{i - \alpha_{cte}}{n - 2 \times \alpha_{cte} + 1}$$

onde $n$ é o número de observações na amostra e $\alpha_{cte}$ é uma constante pré determinada.

</blockquote>

Todas as referências que encontrei indicam o uso de $3/8$ como valor correto para esta constante. @Blom1958 estudou três constantes ( $0$, $3/8$ e $1/2$), chegando a conclusão de que $3/8$ apresenta resultados mais precisos, especialmente para dados com poucas observações.

Ambos os trabalhos de @RyanJoiner1976 e @LooneyGulledge1985 utilizam $\alpha_{cte}=3/8$. Assim, ambos testes de Normalidade calculam a estatística com base nos mesmos dados, e, portanto, a estatística dos dois testes é *idêntica*. Portanto, a única diferença entre os testes se deve aos valores críticos.

<br>

### Comparando os valores críticos


@RyanJoiner1976 estimaram valores críticos para $1\%$, $5\%$ e $10%$ de significância, e reportaram equações para cada valor de $\alpha$:

<blockquote>
$$\begin{align}\begin{aligned}R_{critico}^{\alpha=10\%} = 1.0071 - \frac{0.1371}{\sqrt{n}} - \frac{0.3682}{n} + \frac{0.7780}{n^{2}}\\R_{critico}^{\alpha=5\%} = 1.0063 - \frac{0.1288}{\sqrt{n}} - \frac{0.6118}{n} + \frac{1.3505}{n^{2}}\\R_{critico}^{\alpha=1\%} = 0.9963 - \frac{0.0211}{\sqrt{n}} - \frac{1.4106}{n} + \frac{3.1791}{n^{2}}\end{aligned}\end{align}$$

onde $n$ é o tamanho da amostra.

</blockquote>

\* As equações acima foram obtidas a partir de dados simulados e depois de terem sido suavizados. 


Já @LooneyGulledge1985 estimaram valores críticos para uma maior variedade de níveis de significância, cobrindo todo o intervalo de forma espaçada (os mesmos valores estimados por @Filliben1975). O artigo original trás os valores críticos para os diversos níveis de significância em função do tamanho amostral (entre 3 e 100).


A Tabela \@ref(tab:tcriticos) apresenta os valores críticos para $5\%$ de significância variando o tamanho amostral entre $`r n_min`$ e $`r n_max`$ (valores críticos de @RyanJoiner1976 foram arredondados na terceira casa decimal para ter precisão similar aos dados de @LooneyGulledge1985). Como é possível observar, os valores críticos são muito parecidos, sendo iguais em alguns valores de $n$.


```{python, echo=FALSE}
df = pd.concat(all_values)
df_critical_table = df[df[alpha_name] == alpha].copy()
df_critical_table.drop(alpha_name, inplace=True, axis=1)
df_critical_table = df_critical_table.loc[
    df_critical_table[n_sample_name].isin(np.arange(n_min, n_max + 1))
]
```


```{r tcriticos, echo=FALSE}

data <- data.frame(py$df_critical_table)
row.names(data) <- NULL
kable(data, caption = "Comparativo entre os valores críticos dos testes de Ryan-Joiner e Looney-Gulledge", align = "c", col.names = gsub("[.]", " ", names(data))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```


Esta semelhança fica mais evidente quando plotamos o gráfico com todos os valores disponíveis, como apresentado na Figura \@ref(fig:criticalplot). Com exceção de alguns poucos pontos, os valores críticos são iguais.


```{python criticalplot, echo=FALSE, fig.cap='Comparativo entre os valores críticos do testes de Ryan-Joiner e Looney-Gylledge'}
fig, axes = plt.subplots(1, 3, figsize=(10, 3))
for alpha_value, ax in zip(alphas, axes):
    df_aux = df[df[alpha_name] == alpha_value].copy()
    ax.scatter(
        df_aux[n_sample_name], df_aux[rj_name], label=rj_name, color="blue", alpha=0.5
    )
    ax.scatter(
        df_aux[n_sample_name], df_aux[lg_name], label=lg_name, color="red", alpha=0.5
    )
    ax.set_title("$\\alpha=" + f"{round(alpha_value*100)}" + "\\%$")


ax.legend(loc=4, fontsize=8)

fig.supxlabel(n_sample_name)
fig.supylabel(critical_name)
fig.tight_layout()
plt.show()

```


<br>


Poderia parar aqui esta análise, pois a Figura \@ref(fig:criticalplot) deixa bem claro que os valores críticos são muito parecidos. Porém, ainda tenho café na caneca. Para explorar um pouco mais, a Figura \@ref(fig:diffplot) traz a diferença estimada para os valores críticos:


```{python diffplot, echo=FALSE, fig.cap='Comparativo entre as diferenças dos críticos entre os testes de Ryan-Joiner e Looney-Gylledge.'}
fig, axes = plt.subplots(1, 3, figsize=(10, 3))
y_lims = 0
for alpha_value, ax in zip(alphas, axes):
  
    df_aux = df[df[alpha_name] == alpha_value].copy()
    ax.scatter(df_aux[n_sample_name], df_aux[diff_name], color="blue", alpha=0.5)
    ax.set_title("$\\alpha=" + f"{round(alpha_value*100)}" + "\\%$")
    ylims = ax.get_ylim()
    y_max = np.max(np.abs(ylims))

    if y_max > y_lims:
        y_lims = y_max

for ax in axes:
    ax.set_ylim(-1.05 * y_lims, y_lims * 1.05);

fig.supxlabel(n_sample_name)
fig.supylabel(diff_name)
fig.tight_layout()
plt.show()

```


<br>


Os resultados obtidos indicam uma pequena diferença entre os resultados, especialmente para $\alpha=1\%$ e valores de $n$ pequenos. Nos demais casos, a diferença (que já era pequena) é praticamente nenhuma. Para saber quanto, em média, os valores críticos são diferentes, estimei a diferença quadrática média através da seguinte relação:

<blockquote>
$$dff^2 = \sum_{i=3}^{n}\dfrac{\left(RJ_{critico} - LG_{critico} \right)^{2}}{n-1}$$

onde $RJ_{critico}$ é o valor crítico do teste de Ryan-Joiner, $LG_{critico}$ é o valor crítico do teste de Looney-Gulledge e $n$ é o número de valores críticos adotados

</blockquote>

e então a raiz quadrada da diferença quadrática média, obtendo algo parecido com o desvio padrão (Tabela \@ref(tab:tabledesvios)):

<blockquote>
$$dff=\sqrt{dff^{2}}$$

</blockquote>



```{python, echo=FALSE}
df_quadract = []
for alpha_value in alphas:
    df_aux = df[[diff_name, alpha_name]].copy()
    df_aux = df_aux[df_aux[alpha_name] == alpha_value]

    df_quadract.append(
        pd.DataFrame(
            {
                alpha_name: [alpha_value],
                diff_quadract: [np.square(df_aux[diff_name]).sum() / (df_aux.shape[0] - 1)],
            }
        )
    )

df_quadract = pd.concat(df_quadract, ignore_index=True)
df_quadract[desv_pad_name] = np.sqrt(df_quadract[diff_quadract])
```


```{r tabledesvios, echo=FALSE}

data <- data.frame(py$df_quadract)
row.names(data) <- NULL
kable(data, caption = "Média dos desvios entre os valores críticos do teste de Ryan-Joiner e Looney-Gulledge.", align = "c", col.names = gsub("[.]", " ", names(data))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```


<br>

As diferenças obtidas são muito pequenas, porém é bem claro que para $\alpha=1\%$ ela é um pouco maior. Como a equação utilizada para estimar as diferenças as eleva ao quadrado, o sinal não é considerado. Contudo, para esta análise, o sinal da diferença é importante pois os valores críticos indicam o critério de rejeição de $H_0$. Assim, caso um teste apresente consistentemente valores maiores do que o outro teste, temos um padrão e este teste seria mais sensível. 

Como a diferença esta sendo calculada entre o valor crítico do teste de @RyanJoiner1976 e @LooneyGulledge1985 (e.g., $RJ_{critico} - LG_{critico}$), se o resultado é positivo quer dizer que o valor crítico de Ryan-Joiner é *maior*, e portanto será mais rígido (quanto maior o valor crítico, maior a chance de rejeitar $H_0$). A Figura \@ref(fig:barplot) abaixo traz o resultado da análise entre quantas diferenças foram positivas, negativas ou nulas.


```{python barplot, echo=FALSE, fig.cap='Comparativo entre as diferenças dos críticos entre os testes de Ryan-Joiner e Looney-Gylledge'}

def values(x):
    if x < 0:
        return negative_name
    elif x == 0:
        return zero_name
    else:
        return positive_name

dffs = []
for alpha_values in alphas:
    df_aux = df[df[alpha_name] == alpha_values].copy()
    df_aux[diff_name] = df_aux[diff_name].apply(values)

    # print(df.head())

    df_aux = (
        pd.Series(df_aux[diff_name]).value_counts()
        .rename_axis(diff_name)
        .reset_index(name="counts")
    )
    df_aux[percentage_name] = df_aux["counts"] * 100 / df_aux["counts"].sum()
    df_aux[alpha_name] = [alpha_values] * df_aux.shape[0]
    dffs.append(df_aux)

df_positive_negative_perncetages = pd.concat(dffs)

fig, ax = plt.subplots(figsize=(8, 4))
ax = sns.barplot(
    data=df_positive_negative_perncetages,
    x=alpha_name,
    y=percentage_name,
    hue=diff_name,
    ax=ax,
    legend=True,
    palette="bright",
)
ax.set_xlabel(None)
ax.set_ylabel(percentage_name)
for lab in ax.containers:
    ax.bar_label(
        lab,
        labels=[f"{x:,.1f}%" for x in lab.datavalues],
        padding=2,
        rotation=0,
        fontsize=8,
    )
ax.set_xlabel(alpha_name)
ylims = ax.get_ylim()
ax.set_ylim(ylims[0], ylims[1] * 1.05);
sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))
fig.tight_layout()
# plt.savefig("dist_plot.png")
plt.show()
```


<br>

Mais de $65\%$ das diferenças são nulas para dados com $5\%$ e $10\%$, indicando a similaridade entre os testes testes nestes níveis de singinificância. Já para $1\%$ temos que  $67\%$ das diferenças são ***positivas***, indicando que neste caso os valores críticos do teste de @RyanJoiner1976 são ***maiores*** do que os valores críticos @LooneyGulledge1985. 


## Conclusão

Nesta análise comparamos os testes de @RyanJoiner1976 @LooneyGulledge1985. Vimos que a estatística de ambos os testes é estimada de forma idêntica. Depois, comparamos os valores críticos destes testes, e foi possível observar que eles são muito parecidos.

Entretanto, uma avalição mais aprofundada indicou em pequena diferença positiva entre os valores críticos para $1\%$ de significância, especialmente para amostras de tamanho pequeno. Isto mostra que os valores críticos do teste de @RyanJoiner1976 são, em sua maioria, maiores do que os respetctivos valores do teste de @LooneyGulledge1985. 

Do ponto de vista prático isto quer dizer que se o pesquisador decidir adotar 99% de confiança para o erro de tipo I, o teste de @RyanJoiner1976 será um pouco mais rígido do que o teste de @LooneyGulledge1985 (irá rejeitar a hipótese nula mais vezes).

Porém, como na prática nós adotamos 95% de confiança, não fará diferença adotar um ou outro teste. Mas claro, você deveria utilizar o teste de @ShapiroWilk1965, que é o menos pior dentre os testes de Normalidade.




## Referências

<br>